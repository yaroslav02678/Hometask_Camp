Леді і джентельмени. 

## [ ВСТУП ]

Після того як ви трохи підготувалися ознайомившись із класифікацією на попередніх лекціях ми перейдемо до кластеризації.<br> 
Ідея кластеризації полягає в тому, що інтелект це здатність групувати подібні об'єкти.<br> 
Тож теза або ідея проста - якщо ви можете групувати подібні об'єкти це означає, що ми маємо певний рівень інтелекту.<br> 
Тож якщо ви маєте  різні типи геометричних фігур<br> 
та можете згрупувати їх в разом за розміром та кольорами то це означає що ваший алгоритм має певний рівень інтелекту.<br> 
 <br> 
Тобто по суті кластеризація групує немарковані дані в кластери схожих входів.<br> 
І з точки зору навчання та з точки зору будь-якого методу кластеризації ми маємо справу з<br> 
неконтрольованим навчанням. Тож немарковані дані дорівнює неконтрольоване навчання. <br> 
До цього ми опирались лише на контрольване навчання.<br> 

## [ ПРИКЛАД КЛАСТЕРИЗАЦІЇ ]

Давайте подимось на приклад.
Якщо у мене є кінські сили, і я маю максимальну швидкість, в якості моїх фіч і я хочу класифікувати автомобілі<br> 
я можу отримати щось подібне до того що ви бачите на картинці.<br> 
Я можу отримати певний тип автомобілів у групі кожного кольору. <br> 
Деякі мають багато кінських сил, але максимальна швидкість не дуже висока<br> 
Є автомобілі, які не мають багато кінських сил але більш-менш така ж сама<br> 
максимальна швидкість. І звичайно є автівки що не маютьбагато кінських сил які вони насправді швидкі.<br> 

Кластеризація в основному йде і знаходить<br> 
групи об'єктів, які дуже схожі і до цих групи можуть належати як і вантажівки<br> 
так і спортивні автомобілі чи позашляховики, так що звичайно ніякої техніки яка сказала б нам, який саме це тип автомобіля<br> 
вони просто скажуть, що це група один, група два, група три та група чотири. <br> 

## [ РІЗНИЦЯ МІЖ КЛАСЕТРИЗАЦІЄЮ ТА КЛАСИФІКАЦІЄЮ ]

Ви можете призначити кластеру лейбл і сказати, що група один - це позашляховики, це<br> 
не має значення який лейбел ми надамо. Коли ми говоримо про лейбл ми маємо на увазі лейбл з<br> 
точки зору що у нас є лише вхідний вектор фіч, на відміну<br> 
від контрольованого навчання (класифікації) де ви отримуєте<br> 
крім вхідного вектора ще і бажаний результат. Саме тому при роботі із кластеризацією <br> 
ми не повинні поводитись так саме як під час контрольованого навчання (Тобто вибирати лейбл який будемо передбачати).<br> 
В основному ви даєте кожен пункт представлений в цьому<br> 
спрощеному варіанті з двома характеристиками<br> 
( кінські сили, максимальна швидкость),а потім якийсь алгоритм кластеризації приходить і<br> 
групує дві точки разом і каже що це одна група це одна група, а друга група це <br> 
друга група. Я не знаю як вони називаються, я поняття не маю до якої категорії авто вони належать, <br> 
але я знаю що вони зхожі між собою і це той рівень інтелекту який пропонує нам кластеризація.<br> 

## [ ЩО НАС ЦІКАВИТЬ В ДАННИХ ]
 
Коли ми починаємо працювати з кластеризацією нам було б добре запитати наступні питання. <br> 

1. Чи наші кластери добре відокремлені ?<br> 

У нашому дитячому прикладі нам чітко видно, що<br> 
класи автомобілів добре відокремлені. Відстані між ними доволі помітна так, що я можу дати це<br> 
найдурнішому алгоритму і він буде<br> 
здатний розпізнати класи. Це легка задача. Ці кластера добре відділені один від одного<br> 

Наступне питання яке нас цікавить це:<br> 

2. Чи лінійно відокремлені наші класетра?<br> 
Хоча я не малюю тут ніяких ліній, але якщо я<br> 
захочу то чи міг би я відокремити ці групи за допомогою малювання якихось<br> 
ліній? <br> 
Як що мені легко намалювати лінії то це лінійно відокремлювана<br> 
проблема. <br> 
Це не має нічого спільного з тим чи<br> 
ви бажаєте класифікувати дані, коли ви хочете<br> 
кластеризувати дані. Абсолютно нічого спільного. Лінійна відокремлюваність говорить про те, наскільки<br> 
важко розпізнати речі.Якщо лінійно відокремлені то це легко, якщо вони<br> 
не є лінійно відокремлюваними, це<br> 
важко. Як бачите на другій картинці у мене немає можливсоті намалювати пряму лінію  і якось відокремити кластера.<br> 
<br> 
Все ж таки незалежно від того, що саме ми хочемо робити із данними. нам буде дуже корисно розуміти те чи є дані простими або складними,<br> 
як саме впливають вони на нас, І які складнощі ми можемо очікувати у майбутньому.<br> 

## [ ПОТЕНЦІЙНІ ВИКЛИКИ ]
Складнощі можуть бути наступні<br> 

1. Накладання, кластери  перекривають один одного так що вони заходять всередину  <br> 
території один одного.<br> 
2. Також ви можете отримати складні<br> 
форми. Форма кластерів може не бути простою.<br> 
У навчальних прикладах ви завжди бачите чітку кругло (або майже круглу) форму кластерів.<br>  
В реальному світі у нас не жавжди виходять такі хороші кластера.<br>  

## [ ТИПИ АЛГОРИТМІВ КЛАСТЕРИЗАЦІЇ ]

Отже алгоритми кластеризації можна<br> 
розділити в основному на два типи відносно того що вони від нас очікують.<br> 
Перший тип це ті які повинні знати кількість кластерів.<br> 
Хто мені сказав що я маю шукати для чотирьох кластерів ? Я просто це знав.<br> 
Тому я сказав алгоритму шукати чотири кластера. Так що буде якщо я попросити алгоритм кластеризації<br> 
шукати п'ять кластерів чи шість ? Він знайде щось, що скоріш за все це не матиме для нас сенсу.<br> 
Тому є методи, де ви маєте сказати їм яку кількість кластерів шукати.<br> 

Є і такі, що не потребують цього. Вони самі зясовують те скільки кластерів вам підійде. <br> 
<br> 
І це дуже важливий атрибут техніки кластеризації. <br> 
Якщо мені не потрібно  вказувати алгоритму скільки кластерів шукати то це може стати<br> 
дуже дуже складним підходом для нас. Тому що здебільшого ми не працюємо<br> 
з двома фічами здебільшого ми працюємо з кількома сотнями фіч і<br> 
здебільшого ви навіть не уявляєте як багато різних паттернів у данних.<br> 

Тож як нам дізнатись, а скільки саме класетрів нам потрібно ? <br> 
Що ж, у нас є різні трюки і техніки для цього, але ми поговоримо про них трішки пізніше.<br> 

## 
[ K-Means ВСТУП ]

Ми починаємо нашу подорож із знайомства із K-Means. 

K-Means було винайдено на початку 70-х років. Із того часу зявилось дуже багато варіацій цього алгоритму<br> 
Але основна ідея залишається незмінною. А головна ідея K-Means — це знаходження центроїдів, які в основному є<br> 
протипами або середнім значенням наших кластерів.<br> 
Отже K-Means в основному означає, скільки означає<br> 
у вас є скільки середніх значень. У нашому прикладі із автомобілями у нас є чотири кластери.<br>  Це означає що нам потрібно чотири середніх 
значення(центроїдів) для цих кластерів. Тобто k означає як багато кластерів ми хочемо знайти.<br>  Якщо ви спробуйте знайти прототип для
кожної з цих груп
то прототип виявиться середнім значенням для всього класу. Оскільки ми говоримо про середні значення то це означає що ми можемо лише працювати 
лише із числовими данними. Тобто ніяких данних категорій. Це стосується OneHotEncoder та  CategoricalEncoder.<br> 
Хіба, що ви якось прийдете до рішення як це можна втілити. <br> 

## [ ПОЯСНЕННЯ РОБОТИ K-MEANS ]

<b> Крок 1. </b>
Давайте розберимось як це працює. <br> 
Усе насправді досить просто. спочатку ми випадковим чином ми випадково розміщуємо K центроїдів <br> 
тому, якщо хтось сказав вам, знайдіть три кластерів тут, це означає, що у вас мусить бути три центроїда.<br> 
По факту це будуть просто три рандомні числа, які ми поки не можемо називати центроїдами. <br> 
Нехвилюйтесь вони стануть центроїдами під час другої ітерації.<br>  А поки це лише три рандомні числа. 
Коли ви починаєте з трьох випадкових чисел, вони можуть бути повністю
взагалі за межами реальними данних.<br>  Тоді чому ми це робимо ?
Тому що ми і гадки не маємо про те, з чого нам потрібно почати. 
Саме тому ми це зробим випадковим чином.<br> 
Це дуже схоже із тренуванням нейронної мережі.<br>  Коли у нас величезна кількість ваг і ми не знаємо як їх налаштувати. <br> 
Памятаєте що ми робимо з ними на початку ? Рандомно ініціалізуємо.<br>  Таку саме логіку ми переслідуємо і з центроїдами.<br> 

**Крок 2.**

Ми призначаємо кожну точку данних до найближчого до неї центроїду. <br> 
Як ви бачите на картинці ось ці три точки близькі до центроїда 1. Інші три точки близькі до центроїда 2.<br> 
Коли порахували відстань для кожної точки і знаємо те який центроїд до неї знаходиться найближче<br> 
ми формуємо якийсь початковий кластер. Зазвичай на початку виглядає так що в цих кластерах намеє ніякого сенсу,<br> 
але згодом впродовж ітерацій ваші класери почнуть приймати чітку форму.<br> 

**Крок 3.** 

У нас не буде ніякого навчанняб якщо я зроблю<br> 
залишусь із випадково призначеними центроїдами, тому ми<br> 
повинні рухатися. <br> 
Це і є наший наступний і найважливіший крок. Оновлення центроїдів. <br> 
<br> 
Поглянте на картинку. Рух означає, що цей цей центроїд який був призначений випадковим чином<br> 
мусить якось дістатись свого кінцевого місця. У деяких можу бути проста траєкторія, а в інших довга і складна.<br> 
Якщо мені вдасться це зробити, так аби випадкові центроїди дістались свої місць то ура, я маю 
хороші кластери.<br> 
Тобто частина навчання полягає в тому, аби 
траєкторія цих трьох чисел дісталась до фактичного середнього значення кожного кластера.<br> 
Ми вже порахували відстань для признечення нашої точки данних до класетру. <br> 
Нам потрібно також порахувати відстань у середині кожного класу для призначення нового цетнроїда. <br> 
Ми завжди робимо припущення що наш цетнроїд - хороший. <br> 
Для цього ми рахуємо відстань від нього до кожної точки данних. <br> 
Далі ми це якось додаємо і отримуємо нашу суму. <br> 
Якщо у нас дійсно хороший центроїд це означає, що наша сума має бути мінмальною. <br> 
Якщо ви подивитесь на цю картинку то побачите що центроїди знаходяться перевжно в центрі кластеру. <br> 
Тобто дійсно хороший центроїд буде мати мінімальну відстань до всіх елементів в кластері. <br> 
Якщо зробимо крок в перед, і знайдемо центроїд сума відстаней якого ще менша ніж у нас була то це означатиме що наш попередній
не є оптимальним центроїдом і мусимо рухатись вперед.<br> 
Ми рухаємось вперед і бачимо що похобка зменшується.<br>  Що таке похибка ? Це сума відстаней 
про яку ми говорили раніше. <br> 

Тобто для знаходження нового центроїда ми просто беремо середнє значення координат всіх точок у кластері.<br>  рахуємо похибку. 
Зрівнюємо її із нашим акутальним центроїдом і тоді приймаємо рішення про оновлення.<br> 
Це основна логіка роботи алгоритму K-Means. <br> 

Якщо ви візуалізуєте роботу вашого алгоритму на кожній ітерації ви побачите приблизно таку картинку.<br>  (показати гіфку)

## [ ДЕТАЛЬНІШЕ ПРО АСПЕКТИ K-means ]
 
А зараз я ще хочу розповісти вам трішки більше детальніше про атрибути цього алгоритму.<br> 

Отже ми знаємо що схожість визначається відстаннею. <br> 
Тобто у нас є центроїд і точка данних. Як ми будемо рахувати відстань ? Усе просто. <br> 
Нам на допомогу приходить стара добра евклідова відстань.<br>  Думаю формулу евклідової відстані пояснювати не потрібно.<br> 
Я припускаю що ви її досі памятає із шкільної програми.<br> 
Якщо відстань між точками маленька - то вони схожі. І навпаки якщо відстань між точками велика то вони не схожі. <br> 
Тобто ми дуже довіряємо формулі евклідової відстанні. Вона практично і є двигуном усього алгоритму.  <br> 

Наша кінцева ціль це похибка, а саме її мінімізація.<br> 

Похибка це сума відстаней від усіх точок до центрів тих кластерів, до яких ці точки належать. <br> Ось тут записана точна формула. <br> 
Ця сумма має бути дуже маленькою в кінцевому результаті. Тому ми і дивимось на неї як на похибку.<br>  Хоча технічно це не похибка,
але значення відстані говорить нам про те чи схожі дві точки між собою - тому ми кажемо що це похибка. <br> 

Останння формула це формула для знаходження нового центроїду.<br>  Ця формула рахує середнє значення координат всіх точок у кластері.<br>  
І це середнє значення і буде нашим новим центроїдом. <br> 

Евклідова відстань - це сама поширена відстань.<br>  Вона використовується за замовчування в велечезній кількості підходів. <br> 
Вона проста і інтуїтивно зрозуміла.<br>  Але я хочу наголосити для вас, що формула евклідової відстані не є константою  і ви можете експерементувати
із цим параметром.<br>  Зміна формула обрахування відстаней змінить саму логіку того як формуються кластера. 

Для прикладу. <br> 
Якщо у вас вектори по 100 фіч. То можете сміло залишатись із евклідовою відстаню. Для вас цього буде достатньо. <br> 
Якщо у вас вектори по 10 тисяч фіч то евклідова відстань смокче. Вона вам ніяк не допоможе тут.<br>  Це занадто багато. І евклідова просто не дасть надійний результат. <br> 
В такому випадку ви можете спробувати використовувати косиносну схожіть. Або зменшувати розмірність за допомогою (PCA). <br> 

Невеличкий відступ. PCA (Principal Component Analysis) - це техніка яка лінійно зменшує розмірність. <br> 
У вас буде по ній окрема лекція тому зараз я не буду сильно заглиблюватись до цього. <br> 

Якщо у вас вектори мають мільйони фіч - то можливо K-Means не підходить для ваших задач. <br> 

Так саме є ще інші відстані знання яких безумовно знадобиться вам. <br> 
 
- Відстань Хеммінга
- Відстань Джакарта

Ці відстані вони дозволяють вам в певній мірі кластеризувати категоричні типи фіч. <br> 
На них я не буду загострювати свою увагу, і залишу вам це для самомстійного вивчення. <br> 

## [ КОЛИ ЗУПИНИТИСЬ ]
 
Добре, із тим як оновлювати центроїди ми розібрались. Ми знаємо як рухатись вперед і як знаходити хороші центроїди
для наих кластерів. Це все дуже добре, але тепер нам потрібно розібратись коли нам слід зупинитись.<br>   скільки Ітерацій для нас достатньо ?<br> 

Коли ми повинні сказати: "Все ми найшли хороші центри, стоп" ?. <br> 

Схожі питання виникають і коли ми тренуємо нейронні мережі. <br> 

Отже ми можемо зупинитись після певної кількості ітерацій. <br> 
Припустимо, що ми вважаємо що 5000 тисяч ітерацій для нас достатньо. <br> 
Ми просто беремо те що маємо і працюємо із результатами.<br> 

Другий випадок випадок це коли ми можемо зупинитись - це тоді коли наші центроїди не змініються,
або просто не змінюються значно.<br>  Наприклад якщо ви візуалізуєте те як змінюються ваші центроїди протягом ітерацій то<br>  
ви можете спостерігати як вони поступово наближають до центру кластера, а потім просто трішки міняються свою позицію<br> 
туди-назад.<br>  Це означає що наш центроїд дома, тому ми в насутпних ітерація немає сенсу. <br> 

Третій випадок це практично те саме що і друге.<br> 
Коли ви рандомно ініціалізуєте кластера і візуалізуєте роботу K-Means ви будете спостерігати як 
ваші данні міняють свій кластер.<br>  Подивіться на картинку.<br>  У нас є два дата поінти між кластерами.
Нехай один клас це вантажівки а інший позашляховики.<br>  І протягом великої кількості ітерацій ми можемо бачити як
вантажівка стає спорткаром, а спорт кар вантажівкою. Ви чуєте як тупо це звучить ? <br> 
Але що вдієш ми робимо подібні помилки.<br> З часом ви помітити, що більше ваші данні не міняють кластера, або просто декілька датапоінтів 
постійно стрибають між двома кластерами.<br>  Це такі випадки де ми не впевнені точно.<br>  Тут далі немає сенсу продовжувати
і якщо таких дата поінтів у вас буквально одиниці то просто прийміть це як потенційні помилки і рухайтесь далі. <br> 

## [ ПРОБЛЕМИ K-MEANS ]

Найбільшою проблемою K-MEANS є те що вона потребує К на вхід. Тобто знати скільки кластерів шукати. <br> 
Можливо я не знаю скільки скільки мені потрібно ? Хіба це не частина контрольованого навчання ? Частина.<br> 
Алгоритм не настільки розумний, аби сам визначити кількість кластерів. <br> 
Проте він чудово розпізнає кластери коли йому вказують їх кількість.<br>  А якщо ми не можемо цього зробити
то це рішення для нас не підходить. <br> 

Друга проблема.<br> 
K-Means дуже чутливий до аутлаєрів. <br> 
Аутлаєр це коли у нас є дата поінти які виділяються із загального розподілу данних. <br> 
Подивіться на картинку. У нас дата-поінти тут, дата-поінт тут і дата поінт ось тут. <br> 
Це оутлаєр. Такі випадки просто зводять K-means з розуму.<br> 
Тобто якщо ви знаєте що ваші данні мають аутлаєри, то ви повинні бути обережні використовуючи K-Means.<br> 

І третя проблема. <br> 

K-Means. Робить так званий hard-clustering. <br> 
Поясню вам це на прикладі. Подивіться на картинку. <br> 
У нас є два кластери. Уявімо що наш датапоінт це вектор який має два значення<br> 
які кажуть що він нажелить до кластера 1 із іммовірністю 90% і до кластера два із іммовірністю 10%, <br> 
Але це не те що робить K-MEANS.  K-MEANS каже нам просто ТАК або НІ. І це називається хард кластеринг.<br> 
Чому це шкідливо для нас ? Тому що маючи такий бінарний розподіл (1 та 0) ми втрачаємо дуже багато інформації. <br> 
Подивіть на ось це приклад. У нас тут данні у вигляді метелика. Для бокових точок очевидно до якого кластера вони належать.<br> 
А до якого класетра належить центральний дата поінт ? До 1 чи до 2 ? Тут нам дуже важко сказати. <br> 
Що нам робити в такому випадку ? Кидати монетку ? Вгадувати ?. Ну К-means по суті так і буде робити, через те <br> 
що він мусить сказати що це або класетр 1 або кластер 2. Хоча інколи для нас буде корисно мати можливість сказати "Я не знаю куди нажелить ця точка"<br> 
Тобто при хард кластерингу у нас значення нашого  вектора будуть [ 0,1 ]при софт кластерингу вони будуть [0.5, 0.5].<br>  
Тобто K-MEANS не може вам дати тут чесну відповідь. <br> 

Але не хвилюйтесь в майбутніх лекціях вам розкажуть про техніки які можуть це зробити. <br> 

## [ ПРАКТИЧНЕ ЗАВДАННЯ ]

А тепер я покажу вам трішки практики. Аби продемонструвати вам як це все працює на практиці. <br> 
За основу я взяв перший датасет про машини який попався мені під руку на каглі. <br> 
І на його основі я буду демонструвати вам роботу K-means. <br> 

## [ ОГЛЯД ДАННИХ ]

Як бачите це датасет про елітні автівки. <br> 
У нас є купа характеристик (числові данні) і також данні категорій. Наприклад бренд чи країна вироюник. <br> 
взагалі-то цей датасет використовувався для класифікації брендів та регресії вартості автомобіля. <br> 
Ну а ми будемо запихати його в K-means. <br> 

## [ DATA PROCESSING ]

Я не буду мучити вас кореляційними матрицями і якимось початковим аналізом данних. <br> 
Ви цьому навчились на попередніх лекціях тому спробуєте втілити свої знання у домашньому завданні.  <br> 
Я зроблю максимально ліниво і просто відберу усі числові типи фіч які в мене є. <br> 
Далі  я використаю PCA для того аби перетворити усі мої 17 фіч в 3 PCA компоненти. <br> 
Про те що це таке і навіщо його використовують вам розкажуть на наступних лекціях. <br> 
Я ж використовую його, бо я дуже люблю PCA. Тільки уявіть що ви можете стиснути 17 фіч в 3 фічі. <br> 
І при цьому не втратити репрезантативності в данних.<br> 

А тепер давайте подивимось на візуалізацію наших данних. <br> Ось в принципі так Зазвичай будуть виглядати ваші данні. <br> 
Абсолютний хаос і абсолютне нерозуміння того скільки в нас є патернів і скільки кластерів нам потрібно. <br> 

Тож як нам розібратись із кількістю кластерів ? <br> 
Тут я продемнострую вам одну техніку для цього. І називається вона метод силуета. <br> 

## [ МЕТОД СИЛУЕТА ]

Метод силуета це кінцева метрика по якій ми розуміємо наскільки хороші у нас кластера. <br> 
Трюк полягає в тому що ви рахуєте цю метрику для певного діапазону кластерів. Скажімо від 4 до 10.<br>  
І на основі цієї метрики ви вирішуєте скільки кластерів вам потрібно. <br> 

А тепер трішки детальніше до того що це за метрика.<br> 

Метод силуета — це спосіб оцінити якість кластеризації для кожної точки в датасеті, а також для всієї моделі в цілому.<br> 
Він показує, наскільки добре кожен об’єкт належить до свого кластеру, порівнюючи його відстань до об'єктів у своєму кластері
та до найближчого сусіднього кластеру.<br> 

a(i) — середня відстань від точки i до всіх інших точок у тому ж кластері (внутрішня згуртованість).<br> 

b(i) — найменша середня відстань від точки i до всіх точок в іншому кластері (міжкластерна роздільність).<br> 

Після цього ми рахуємо загальну оцінку. Наше фінальне велике С. Чим вище це С тим кращі наші кластери. <br> 

Для зручності ми побудуємо графік для коефіцієнтів і кількості кластерів. <br> 
Як бачите цей метод показує шо для нас найкраще це 5 або 6 кластерів. Ну це вже щось. <br> 
А отже ми можемо це використовувати як певну вхідну точку до нашого дослідження і рухатись далі. <br> 

## [ КЛАСТЕРИЗАЦІЯ ]

Ось так виглядають кластеризовані данні. <br> 

Прошу помітити що я використовував як правило сторонні бібліотеки. <br> 
Зазвичай ви дуже рідко будете писати ці формули чи алгоритми ручками.<br> 
Особисто я під час роботи лише один раз писав складну математичну логіку з нуля.<br> 
І це лише через те що моя задача була дуже вузького профілю.<br> 

Вам ж звичайно буде корисно написати це самостійно для того аби навчитись. <br> 
Але я вважаю що більш корисним для вас буде зрозуміти суть того що ви робите. <br> 
Я хочу аби ви бачили не просто "інструкцію" як треба робити, а щоб ви бачили логіку у моїх кроках. <br> 

Розуміння цього дозволить вам бути більш гнучкими у ваших задачах. <br> 
Тут немає правильних і не правильних рішень. Є просто спектр хороших і спектр поганих. <br> 
Тому не експементуйте, пробуйте, помиляйтесь, робіть висновки і зчасом туман незвіданого перед вам розсіється. <br> 

А тепер перейдемо до домашнього завдання. <br> 

## [ Home task ]

Вашим домашнім завданням буде покластеризувати Iris датасет. <br> 
Це популярний датасет для навчальних цілей і я підозрюю що ви вже <br> 
працювали із ними під час попередніх лекцій. <br> 
Тим не менш, я думаю що його можна використати і для кластеризації. <br> 

Отже що вам потрібно буде зробити. <br> 
1. Це відібрати фічі для кластеризації. <br> 
2. Зробити поверхневий аналіз данних. Цей пункт не є обовязковим. <br> 
Але це буде плюсом у вашу карму. <br> 
3. Нормалізуйте дані якщо потрібно. Постарайтеся подивитись на дані<br> 
і самостійно прийняти рішеня, чи потрібно вам їх нормалізовувати чи ні. <br> 
4. Знайдіть оптимальну кількість кластерів. Можете використовувати метод силуета, або який небудь інший метод.<br> 
Ви можете навіть написати, що візуально із графіка зрозуміло що вам потрібно 3 кластера. <br> 
Але все таке краще використайте метод де в основі лежать цифри, а не інтуіція. <br> 
5. Створіть кластери. <br> 
6. Скористайтеся візуалізаціями нижче, аби побудувати графіки. <br> 
Подивіться як співвідносяться знайдені кластери до реальних категорій.   <br> 